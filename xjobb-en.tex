\documentclass[a4paper,11pt]{kth-mag}
\usepackage[T1]{fontenc}
\usepackage{textcomp}
\usepackage{lmodern}
\usepackage[latin1]{inputenc}
\usepackage[swedish,english]{babel}
\usepackage{modifications}

\usepackage{graphicx}

% Put title here
\title{Weakly Supervised Object Detection: (Working title)}

\subtitle{Duis autem vel eum iruire dolor in hendrerit in
          vulputate velit esse molestie consequat, vel illum
          dolore eu feugiat null}
\foreigntitle{Lorem ipsum dolor sit amet, sed diam nonummy nibh eui
              mod tincidunt ut laoreet dol}
              
\author{Muhammad Iqbal Tawakal}

\date{2015}
\blurb{Master's Thesis at NADA\\Supervisor: Tjoho\\Examiner: Tjohej}
\trita{TRITA xxx yyyy-nn}

\begin{document}
\frontmatter
\pagestyle{empty}
\removepagenumbers
\maketitle
\selectlanguage{english}

\begin{abstract}
  This is a skeleton for KTH theses. More documentation
  regardings the KTH thesis class file can be found in
  the package documentation.
  
  Weakly supervised object detection is...
\end{abstract}

\clearpage
\begin{foreignabstract}{swedish}
  Denna fil ger ett avhandlingsskelett.
  Mer information om \LaTeX-mallen finns i
  dokumentationen till paketet.

\end{foreignabstract}
\clearpage
\tableofcontents*
\mainmatter
\pagestyle{newchap}


\chapter{Introduction}
\label{chap:intro}

% Computer vision history
Computer vision is a field that focused on the process of acquiring, analyzing, and ultimately extracting knowledge and getting comprehensive understanding of the input image. The input image can take many forms such as still image (black and white or colored image), video sequences, multiple input from multiple cameras, and also input from other related sensor. This field has strong ties with other field such as artificial intelligence, machine learning, and robotic.

Two of the most common typical tasks are object recognition and object detection. Object recognition tries to predict and identify what class of object is contained inside an image. In detection case, not only the class of the object, the position itself must also be located, usually by marking a tight bounding box closely surrounding the object. Both are challenging problems that are still being actively researched today.

% Reason for difficulty
There are many factors that can be attributed as to why the object recognition and its kind are hard. One reason is simply because the natural representation of image as 2D matrix of pixels value hold little to no direct information that can be used to distinguish different object. Figure \ref{fig:car} shows this example. Another factor is the high dimensionsionality of the data. A standard high definition colored image with the size of 1280x720 can contains up to 2764800 dimensions.

\begin{figure}[h]
\centering
\includegraphics[scale=0.5]{image/car.png}
\label{fig:car}
\caption{The image as 2D matrix of pixels.}
\end{figure}

Furthermore, there are many variations that can happen which makes same identical object to have very different appearances in the image. This can be due to viewing changes (translation, rotation, or scale change), change of illumination, occlusion, and so on. There are also intrinsic difference of object from the same class. Figure \ref{fig:objects} illustrates this example.

\begin{figure}[h]
\centering
\includegraphics[scale=0.5]{image/cars.png}
\label{fig:cars}
\caption{Object with different appearances from the same class}
\end{figure}

% The importance of features
All this boils down to finding the best representation or features of the said object. An ideal representation would be highly discriminative of different class of object but captures the intrinsic similarity between object from the same class. It also has to be invariant to the object transformation (translation, rotation, and scale invariant), usually have lower number of dimension, and also fast and easy to compute.

[illustration of features]

% Handcrafted features
Many studies have proposed a wide array of feature extraction techniques. Until very recently, Scale-Invariant Feature Transform (SIFT) and Histogram of Oriented Gradients (HOG) are the leading technique for feature extraction and produced decent performance. Both are similar in the sense that both first get the gradient in the image and then count the gradient based on their direction into a histogram.

[photo of SIFT output]
[photo of HOG output]

Some study extends these features into mid-level representation by clustering nearby features (using k-means clustering for example) and construct the so-called visual word. However, this kind of hand-engineered features still cannot reach the level of human performance.

This trend shifts, there comes the initiative to learn features directly from the data. This is motivated due to the abundance of image data, mostly coming from the internet. There are two different approaches for this, the unsupervised and supervised learning.

Unsupervised feature learning work without label. It tries to learn generic features of object, regardless of their class. It is more advantageous because most of the data comes unlabeled and the process to get the labels can be expensive.

One technique that implement this idea is Sparse Autoencoder. It is a variant of neural network with the number of hidden neuron less than the dimension of the input. It forces the network to learn compressed representation of the image.

\begin{figure}[h]
\centering
\includegraphics[scale=0.3]{image/autoencoder.png}
\label{fig:autoencoder}
\caption{The features learned from MNIST dataset. Each image correspond to edge at different location and orientation.}
\end{figure}

Supervised feature learning works with labeled image. One technique that successfully implement this idea is Convolutional Neural Network (ConvNet). This method is getting popular with the success of the seminal work of Krizhevsky et al that produce stellar performance on the ImageNet large scale recognition problem \cite{krizhevsky2012}. 

Part of this success can also be attributed to the abundance of labeled training data. ImageNet contains 1.2 million training images from 1000 different categories. Another factor is the advance in Graphical Processing Unit (GPU) which makes training large amount of data feasible. Combining these factor with the learning capacity of a ConvNet, makes the result possible.

Further research has showed that the feature produced from this network to be highly discriminative, even with different task and dataset. Study by Razavian et al. \cite{razavian2014} shows that the features coming from the last second fully-connected of ConvNet trained on ImageNet with simple linear classifier such as Support Vector Machine (SVM) produced top and near state-of-the-art performance in many typical vision tasks such as object classification, fine-grained recognition, and image retrieval. 

[Visualization of features learned from ConvNet that generalize well to other dataset]

The result is pushed even further by finetuning the network with the target dataset. Finetuning works by taking the previously trained network, for example, the AlexNet, and retrain it with target dataset. This works particularly well, rather than training the network from beginning, especially if the target dataset has limited number of training data. This kind of learning, transfer learning, has been reported to produce better result on many occasions \cite{azizpour2014}.

One of the latest example that utilize this power of finetuning is the algorithm Region with CNN (RCNN). RCNN is a object detection algorithm that combines the highly discriminative features with a better search strategy. RCNN employs region proposal algorithm, selective search, that return ~2000 regions that is very likely to contain object. These two makes RCNN to get state-of-the-art result on benchmark dataset and made impressive improvement over previous methods.

This thesis work tries to extends the usability of RCNN. The standard RCNN needs the exact bounding box annotation for each object in the image for finetuning. However, this kind of annotation is more expensive than simply labeling the whole image. Therefore, we tries to bypass this needs and figure out a way to finetune with only whole image label. This kind of learning between the fully supervised and unsupervised is dubbed as weakly supervised learning.

The proposed general setup is as follows. First, we use a region proposal algorithm that can give confidence score that shows how likely a region to contain object. There are several possible candidates such as objectness \cite{obj} and EdgeBoxes. Hosang et al. \cite{hosang2014} provide extensive review of popular algorithm and then outline their strength and flaws. We pick top k regions and then extract them into individual image and and give the label from their source, whole image label.

These regions are then used for finetuning ConvNet that has previously been trained using ImageNet 1.2 million training images in 300,000 more iterations. This finetuned network will then be fed to the standard RCNN pipeline. The performance will be assessed using PASCAL 2007 detection dataset. It will be compared to the non-finetuned network, finetuned network with object bounding box annotation, and network finetuned with whole image.


\chapter{Literature Review}
\section{Convolutional Network (ConvNet)}
Convolutional Neural Network (ConvNet) is a variant of a standard neural network.

\section{Region with CNN}

\section{Weakly Supervised Object Detection}
There are wide body of research in the realm of weakly supervised object detection.


\chapter{Experiment and Result}
\section{Dataset}
The dataset used is PASCAL Visual Object Class 2007. This dataset contains nearly 10000 images of 20 different visual classes.

To validate, first we perform the baseline, as done in the RCNN paper.

\bibliographystyle{plain}
\bibliography{ref.bib}

\appendix
\addappheadtotoc
\chapter{RDF}\label{appA}

\begin{figure}[ht]
\begin{center}
And here is a figure
\caption{\small{Several statements describing the same resource.}}\label{RDF_4}
\end{center}
\end{figure}

that we refer to here: \ref{RDF_4}
\end{document}
