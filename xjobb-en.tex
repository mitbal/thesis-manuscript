\documentclass[a4paper,11pt]{kth-mag}
\usepackage[T1]{fontenc}
\usepackage{textcomp}
\usepackage{lmodern}
\usepackage[latin1]{inputenc}
\usepackage[swedish,english]{babel}
\usepackage{modifications}

\usepackage{graphicx}
\usepackage{amsmath}

% Put title here
\title{Weakly Supervised Object Detection: (Working title)}

\subtitle{Duis autem vel eum iruire dolor in hendrerit in
          vulputate velit esse molestie consequat, vel illum
          dolore eu feugiat null}
\foreigntitle{Lorem ipsum dolor sit amet, sed diam nonummy nibh eui
              mod tincidunt ut laoreet dol}
              
\author{Muhammad Iqbal Tawakal}

\date{2015}
\blurb{Master's Thesis at NADA\\Supervisor: Tjoho\\Examiner: Tjohej}
\trita{TRITA xxx yyyy-nn}

\begin{document}
\frontmatter
\pagestyle{empty}
\removepagenumbers
\maketitle
\selectlanguage{english}

\begin{abstract}
  This is a skeleton for KTH theses. More documentation
  regardings the KTH thesis class file can be found in
  the package documentation.
  
  Weakly supervised object detection is...
\end{abstract}

\clearpage
\begin{foreignabstract}{swedish}
  Denna fil ger ett avhandlingsskelett.
  Mer information om \LaTeX-mallen finns i
  dokumentationen till paketet.

\end{foreignabstract}
\clearpage
\tableofcontents*
\mainmatter
\pagestyle{newchap}


\chapter{Introduction}
\label{chap:intro}

% Computer vision history
Computer vision is a field that focused on the process of acquiring, analyzing, and ultimately extracting knowledge and getting comprehensive understanding of the input image. The input image can take many forms such as still image (black and white or colored image), video sequences, multiple input from multiple cameras, and also input from other related sensor. This field has strong ties with other field such as artificial intelligence, machine learning, and robotic.

There are wide range of real world applications for computer vision, starting from simple inspection system in industry and manufacturing process to the creation of robot with artificial intelligence that can interact with  the world around them. Other application include system for navigation (for example autonomous mobile robot and vehicle) and military (security surveillance and missile guidance system).

Those applications can usually be broken down into several small scale vision tasks. Two of the most typical tasks in computer vision are object recognition and object detection. Object recognition tries to predict and identify what class of object is contained inside an image. In detection case, not only the class of the object, the position itself must also be located, usually by marking a tight bounding box closely surrounding the object. Both are hard, challenging problems that are still being actively researched today.

% Reason for difficulty
There are many factors that can be attributed as to why the object recognition and its kind are hard. One reason is simply because the natural representation of image as 2D matrix of pixels value hold little to no direct information that can be used to distinguish different object. Figure \ref{fig:car} illustrate this example. 

This is worsened by the high dimensionality of the data. A standard high definition colored image with the size of 1280x720 can contains up to 2764800 dimensions. Meanwhile the amount of training data rarely exceed or even match this number. This curse of dimensionality can make the training phase to become slow and hard.

\begin{figure}[h]
\centering
\includegraphics[scale=0.4]{image/car.png}
\label{fig:car}
\caption{The image as 2D matrix of pixels.}
\end{figure}

Furthermore, there are many variations which makes same identical object to have very different appearances in the image. This can be due to viewing changes (translation, rotation, or scale change), change of illumination, occlusion, and clutter. There are also intrinsic difference of object from the same class. Figure \ref{fig:objects} illustrates this example.

\begin{figure}[h]
\centering
\includegraphics[scale=0.4]{image/cars.png}
\label{fig:cars}
\caption{Object with different appearances, be it color or shape, from the same class.}
\end{figure}

% The importance of features
All this boils down to finding the best representation or features of the said object. An ideal representation would be highly discriminative of different class of object but captures the intrinsic similarity between object from the same class. It also has to be invariant to the object transformation (translation, rotation, and scale invariant), usually have lower number of dimension, and also fast and easy to compute.

Unfortunately, these kind of ideal features do not exist in reality. In practice, we compromise by striking a balance of using adequate number of training data and using a stronger learning algorithm to compensate for the not-so-ideal features. Figure \ref{fig:feature} shows illustration of the ideal features and the not-so-ideal features.

\begin{figure}[h]
\centering
\includegraphics[scale=0.4]{image/ideal_feature.png}
\label{fig:feature}
\caption{Illustration of ideal and not-so-ideal features. (a) Ideal feature should have better separability from instance from different class and closely located with object form the same class. (b) Not-so-ideal features plot is jumbled all over the place and have low separability. This, however, can be compensated by using stronger classification algorithm.}
\end{figure}

% Handcrafted features
Many studies have proposed a wide array of feature extraction techniques. Until very recently, Scale-Invariant Feature Transform (SIFT) by Lowe \cite{lowe2004sift} and Histogram of Oriented Gradients (HOG) by Dalal and Triggs \cite{dalal2005hog} are the leading technique for feature extraction and produced decent performance on benchmark dataset. Both are similar in the sense that both first compute the gradient in the image and then count the gradient based on their direction into a histogram.

\begin{figure}[h]
\centering
\includegraphics[scale=0.4]{image/hog.png}
\label{fig:hog}
\caption{Illustration of HOG features applied on image of a person.}
\end{figure}

Some study extends these features into mid-level representation by clustering nearby features (using k-means clustering for example). The number of features that fall into a specific cluster is then aggregated into some bag of visual word. However, this kind of hand-crafted features still cannot reach the level of human performance.

The trend shifts, there comes the initiative to learn the features directly from data. This is partly motivated due to the abundance of image data, mostly coming from the internet. There are two different approaches for this, the unsupervised learning and supervised learning.

Unsupervised feature learning works with image without label. It tries to learn generic features of object, regardless of their class. It is more advantageous because most of the data comes unlabeled and the extra effort to annotate the image with labels can be expensive and time-consuming.

One technique that implement this idea is Sparse Autoencoder. In its simplest form, it is a variant of neural network with the number of hidden neuron fewer than the dimension of the input. It then forces the network to learn compressed, low-dimensional representation of the image. Figure \ref{fig:autoencoder} illustrates some example features learned from this network.

\begin{figure}[h]
\centering
\includegraphics[scale=0.3]{image/autoencoder.png}
\label{fig:autoencoder}
\caption{The features learned from MNIST dataset. Each image correspond to edge at different location and orientation.}
\end{figure}

Supervised feature learning works with labeled image. One technique that successfully implement this idea is Convolutional Neural Network (ConvNet). It is originally proposed by LeCunn et al. \cite{lecunn1999} for handwriting document classification. This method is starting to gain momentum with the success of the seminal work of Krizhevsky et al. \cite{krizhevsky2012cnn} that produce stellar performance, by beating previous state-of-the-art method by large margin, on the ImageNet large scale recognition problem \cite{imagenet}. 

Part of this success can also be attributed to the abundance of labelled training data. ImageNet contains 1.2 million labelled training images from 1000 different categories. Another factor is the advance in Graphical Processing Unit (GPU) which makes training large amount of data feasible. Combining these factors with the learning capacity of a ConvNet makes the result possible.

\begin{figure}[h]
\centering
\includegraphics[scale=0.3]{image/cnn.png}
\label{fig:cnn}
\caption{Visualization of features learned from ConvNet.}
\end{figure}
[Visualization of features learned from ConvNet that generalize well to other dataset]

Figure \ref{fig:cnn} shows the example of features learned from a ConvNet, taken from Zeiler and Fergus \cite{zeiler2014} paper. Further research has showed that the feature produced from this network to be highly discriminative, even across different tasks and datasets. Study by Razavian et al. \cite{razavian2014} shows that the features coming from the last second fully-connected of ConvNet trained on ImageNet with simple linear classifier such as Support Vector Machine (SVM) produced top and near state-of-the-art performance in many typical vision tasks such as object classification, fine-grained recognition, and image retrieval. 

The result is pushed even further by finetuning the network with the target dataset. Finetuning works by taking the previously trained network, for example, the AlexNet, and retrain it with target dataset. This works particularly well, rather than training the network from beginning, especially if the target dataset has limited number of training data. This kind of learning, transfer learning, has been reported to produce better result on many occasions \cite{azizpour2014}.

One of the latest example that utilize this power of finetuning is the algorithm Region with CNN (RCNN). RCNN is a object detection algorithm that combines the highly discriminative features with a better search strategy. RCNN employs region proposal algorithm, selective search, that return ~2000 regions that is very likely to contain object. These two makes RCNN to get state-of-the-art result on benchmark dataset and made impressive improvement over previous methods.

This thesis work tries to extends the usability of RCNN. The standard RCNN needs the exact bounding box annotation for each object in the image for finetuning. However, this kind of annotation is more expensive than simply labeling the whole image. Therefore, we tries to bypass this needs and figure out a way to finetune with only whole image label. This kind of learning between the fully supervised and unsupervised is dubbed as weakly supervised learning.

The proposed general setup is as follows. First, we use a region proposal algorithm that can give confidence score that shows how likely a region to contain object. There are several possible candidates such as objectness \cite{obj} and EdgeBoxes. Hosang et al. \cite{hosang2014} provide extensive review of popular algorithm and then outline their strength and flaws. We pick top k regions and then extract them into individual image and and give the label from their source, whole image label.

These regions are then used for finetuning ConvNet that has previously been trained using ImageNet 1.2 million training images in 300,000 more iterations. This finetuned network will then be fed to the standard RCNN pipeline. The performance will be assessed using PASCAL 2007 detection dataset. It will be compared to the non-finetuned network, finetuned network with object bounding box annotation, and network finetuned with whole image.


\chapter{Literature Review}
\section{Convolutional Network (ConvNet)}
Convolutional Neural Network (ConvNet) is a variant of a standard neural network. Here in this section, the development of neural network leading up to ConvNets is briefly described.

\subsection{Artificial Neural Network}
% Single neuron
The first neuron model is proposed by McCulloch and Pitts \cite{mcculloch1943neuron}. This model is a highly simplified version of neuron cell in brain. It works by multiplying the input vector with the weight vector. This value is then fed into an activation function. One activation function that can be used is a step function which acts as some kind of threshold operation. If this value exceeds certain $\theta$, then this network is firing. In other words $ a = \sum x_i w_i $ and:
\begin{equation}
y = 
	\begin{cases}
	1 & \text{if } a \geq \theta \\
	0 & \text{otherwise}
	\end{cases}
\end{equation}

This type of neuron is also called Threshold Logic Unit (TLU). Figure \ref{fig:neuron} illustrates this concept.

\begin{figure}
\centering
\includegraphics[scale=0.5]{image/neuron.png}
\label{fig:neuron}
\caption{Standard model of a neuron}
\end{figure}

To update the the weight of the network to learn from the data, there are two popular update rules, the perceptron rule and the delta rule. Perceptron rule was originally perceived by Rosenblatt \cite{rosenblatt1957} where you just simply add the input vector to the weight vector if the output is misclassified. In other words:
\begin{equation}
\delta w = (t-y) w
\end{equation}
where $t$ is the target output and $y$ is the output of the network.

There is a convergence theorem that states that if the data is linearly separable, then the perceptron learning rule will eventually converge, no matter the learning rate. However, perceptron suffer from the early stopping where if there were no training error, the network stopped learning even though the resulted network still has large validation error.

Delta rule, also known as Widrow-Hoff rule, works by minimizing a loss function. This loss function is defined as the sum of the difference between the target label and the output of the network with linear activation function. The delta rule empirically generalize better than perceptron. The update rule is defined as:
\begin{equation}
\delta w = (t-y) w
\end{equation}

However, this two type of networks can only produce linear classifier. Even some kind of simple non-linear problem such as XOR problem cannot be solved.

% Multi level neural network
One layer of neuron can be stacked to form multi layer neural network. The layer is usually called hidden layer with its neuron called hidden neuron. The input for the next layer is simply the output of the previous layer. It has been shown that this kind of network can be used to approximate all function, given sufficiently enough hidden neuron.

\begin{figure}
\centering
\includegraphics[scale=0.5]{image/multilayer.png}
\end{figure}

The caveat is then how to update the weight of the network, because we only know the error from the last layer. e solution is by using threshold-like function, such as sigmoid function, and then performing chain rule to derive the learning update formula. This update formula is known as Backpropogation.

Two popular choice for sigmoid function is $\frac{1-e^{-x}}{1+e^{-x}}$ or $arctan(x)$. Figure \ref{fig:sigmoid} shows the plotting.

\begin{figure}
\centering
\includegraphics[scale=0.5]{image/sigmoid.png}
\label{fig:sigmoid}
\caption{Sigmoid function.}
\end{figure}

\subsection{ConvNet}A Convolutional Neural Network (ConvNet) is a variant of the multi layer neural network. It is inspired by the visual cortex in retina, where there are small sub-region in the cell that are sensitive, called receptive field.

There are several different paradigms of ConvNet than regular network.
The first is sparse connectivity. A neuron in this case do not need to be connected to all the neurons from the previous layer. For example, a neuron in layer i only need to be connected to limited number of spatially adjacent neuron on layer i-1. This correlates with the receptive field concept.

\begin{figure}
\centering
\includegraphics[scale=0.5]{image/sparse.png}
\label{fig:sparse}
\caption{Illustration between different connection. (a) Densely connected network connect all neuron from previous layer to the current one. (b) Sparsely connected network only connect limited number of neuron. In this case it is 3 spatially adjacent neurons.}
\end{figure}

The second is shared weight. It means that the weight is the same for different parts of the input. This will ensure that the distinct features will be detected regardless of their position in the input. It also reduces the number of parameter that needs to be trained.

\begin{figure}
\centering
\includegraphics[scale=0.5]{image/shared.png}
\label{fig:shared}
\caption{Shared weight. Weight connection with the same color have the same value.}
\end{figure}

This two concepts together ultimately is just a convolution operation of an image with a kernel. Therefore, in ConvNet we are trying to learn the most discriminative kernel that can distinguish different objects instead of designing it by hand.

Another important concept is max-pooling. Max-pooling partitions the image into non-overlapping rectangular region, and for each region, outputs the maximum value. It has the advantage of reducing the computation cost and also provide some form of translation invariance into the network.

There are other design by choice.
First, Rectified Linear Unit (ReLU) as the activation function. $max(a, 0)$. This solves the problem of vanishing gradients. As the network gets deeper, the gradient getting close to zero, which makes the network to hardly learn anything.

\begin{figure}
\centering
\includegraphics[scale=0.5]{image/relu.png}
\label{fig:relu}
\caption{Rectified Linear Unit. It is simply a linear activation function where value below zero is squashed to zero.}
\end{figure}

Dropout, for regularization to prevent overfit. This is done to prevent co-habitation and force the neuron to independently learn the features.

Maybe the most famous ConvNet, the AlexNet, which changed the landscape of computer vision research.
\begin{figure}
\centering
\includegraphics[scale=0.35]{image/alexnet.png}
\label{fig:alexnet}
\caption{The architecture of alexnet.}
\end{figure}

This network is composed of 5 convolutional layers, followed by 2 fully-connected layers and softmax layer to generate prediction.
This model is trained using GPU for 2 weeks with 1.2 million training data and achieved state-of-the-art performance and fire.

This kind of network with more than 1 hidden layer is popular with the name of the so-called deep learning. The main advantage of this system is the first layer can learn primitive feature such as edge and corner, meanwhile the deeper layer can learn the even better layer created from combination of this 

[features learned from ConvNet, layer by layer]

In the past, before ConvNet dominates, a bag of visual word using combined features from HOG, SIFT, and LBP is used.

\section{Object Proposal}
There is a shift in object detection scene. Not so long ago, a densely sampled sliding window is taken at all possible position and scale. It worked well, for example in the case of face detection \cite{violajones}. But when we start to take account general object detection with different aspect ratio, the search space is simply become too big to fully explore. A standard image can produce up to million of windows to be evaluated.

It is usually class-agnostic as it does not care what class is it.

Some notable algorithms are briefly described here.
Objectness \cite{alexe} with multiple cues such as color.

Binarized Normed Gradient (BING) works by learning a generic object identifier in the form that can be generated fast \cite{bing}.

Selective Search \cite{selectivesearch} performs hierarchical grouping from oversegmented region produced from Felzenszwalb and Huttenlocher graph segmentation algorithm \cite{felzenszwalb}. The grouping is done based on multiple criterion. Higher number of proposal is obtained from combining the result from different parameters.

EdgeBoxes work by using and using smart data structure to quickly compute and evaluate possible regions.

Some algorithms provided ranking as some sort of confidence whether the region contains object or not. For a complete treatment of popular object proposal, consult the work of Hosang et al. \cite{hosang}.

\section{Region with CNN (RCNN)}
Region with CNN (RCNN) is the latest object detection algorithm that producing state-of-the-art result on many benchmark dataset such as PASCAL 2007 detection set and ImageNet detection problem. The main contribution of RCNN is by combining the new paradigm of feature learning with finetuning and object proposal.

Domain specific fine-tuning.
Labeled data are scarce. RCNN alleviate this by using supervised pre-training of. The discriminative power

The general framework of RCNN is as follows. First, RCNN use region proposal algorithm Selective Search to generate around 2,000 proposal for each image. Then each region is warped to match the size of the ConvNet network. The features from the last fully-connected layer are then extracted. This features is then classified using pre-trained SVM classifier.

[figure rcnn system]

One benefit of RCNN is the decoupling of feature extraction and region detection. This way, we can use possibly a smarter region proposal algorithm, or use different representation using stronger network.

[some detection result of RCNN]

\section{Weakly Supervised Object Detection}
Object detection that do not need is called weakly supervised object detection. This is for some reason you do not need bounding box annotation which may not readily available. The tedious process can also insert bias and error into the dataset. 

Here are some latest and notable result in weakly supervised world.
Some works by siapaya use hidden topic learning to group subwindows that possibly come from the same category \cite{siapaya}.
Cinbis et al. treat it as multiple instance problem, and used multi-fold approach where the data is divided into train and validation part. The SVM is trained and then it is used to refine the localization of the validation part.

However, due to time constraint, we are only performing the finetuning and treat the SVM classifier with the bounding box annotation. To accommodate, for example by using the work of Cinbis et al. \cite{cinbis} is left for future works.

\chapter{Experiment and Result}
\section{Dataset}
The dataset used is PASCAL Visual Object Class 2007. This dataset contains nearly 10000 images of 20 different visual classes.

To validate, first we perform the baseline, as done in the RCNN paper.

We are using Caffe.
Caffe is designed so an experiment parameter can be changed by changing the parameter prototxt file.
There is a pre-trained model which mimic, with different. It is reported 2\% accuracy different. This network will then be referred throughout the rest of this thesis as caffenet.

As for the RCNN code itself, the author provide it online in a github repository: .

This is the result.
The first result is using the pre-trained caffenet. This result will act as lower bound for the rest of the experiment.

The second experiment is by running RCNN with finetuned network with bounding box annotation. This will act as upper bound.

We then finetune the network with. The strategy is as defined in the paper. Every region with 0.5 is considered as positive samples and.

Because the number of samples can easily reach million of datapoints, negative hard mining is performed.

We tried to build several models. The first, most simple approach is to finetune the network with whole image. In PASCAL, one image can contains many object from different classes.

If we try to include more training image, it is fear that it will corrupt the network with noise.

This might turned out to be.

\section{Discussion}
As can be seen on the. Judging from the plot of loss and accuracy during finetuning, it seems like the network is not learning at all.
This can probably be attributed to the low number of training images.

Another part that has not been done is the weakly supervised learning in SVM side. The SVM training still needs the full bounding box annotations to train.

\bibliographystyle{plain}
\bibliography{ref.bib}

\appendix
\addappheadtotoc
\chapter{RDF}\label{appA}

\begin{figure}[ht]
\begin{center}
And here is a figure
\caption{\small{Several statements describing the same resource.}}\label{RDF_4}
\end{center}
\end{figure}

that we refer to here: \ref{RDF_4}
\end{document}
